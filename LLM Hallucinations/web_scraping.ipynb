{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import contractions\n",
    "from urllib.parse import urlparse\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12451\n"
     ]
    }
   ],
   "source": [
    "with open('gptrc_urls.json', 'r') as file:\n",
    "    # Read the content of the file\n",
    "    gptrc_urls = json.load(file)\n",
    "    \n",
    "print(len(gptrc_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Domains considered as generally reliable for factual information\n",
    "reliable_domains = [\n",
    "    \"harvard.edu\", \"stanford.edu\", \"gatech.edu\", \"uc.edu\", \"cam.ac.uk\",\n",
    "    \"nasa.gov\", \"noaa.gov\", \"fda.gov\", \"gov.uk\", \"europa.eu\",\n",
    "    \"mayoclinic.org\", \"hopkinsmedicine.org\", \"nih.gov\",\n",
    "    \"nature.com\", \"sciencemag.org\", \"nejm.org\",\n",
    "    \"britannica.com\", \"wikipedia.org\",\n",
    "    \"bbc.com\", \"nytimes.com\", \"washingtonpost.com\"\n",
    "]\n",
    "\n",
    "# Function to filter URLs based on the domain reliability list\n",
    "def filter_urls_by_domain(gptrc_urls, reliable_domains):\n",
    "    from urllib.parse import urlparse\n",
    "\n",
    "    # Helper function to check if a domain is in the list of reliable domains\n",
    "    def is_reliable(url):\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc\n",
    "        # Removing 'www.' if present for consistent comparison\n",
    "        clean_domain = domain.replace(\"www.\", \"\")\n",
    "        for reliable_domain in reliable_domains:\n",
    "            if clean_domain.endswith(reliable_domain):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # Filtering URLs\n",
    "    for item in gptrc_urls:\n",
    "        item[\"urls\"] = [url for url in item[\"urls\"] if is_reliable(url)]\n",
    "    \n",
    "    return gptrc_urls\n",
    "\n",
    "# Filter the URLs and update the gptrc_urls list\n",
    "gptrc_reliable_urls = filter_urls_by_domain(gptrc_urls, reliable_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_to_file(results, output_file):\n",
    "    with open(output_file, \"w\") as json_file:\n",
    "         json.dump(results, json_file)\n",
    "    print(f\"Results have been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved to gptrc_reliable_urls.json\n"
     ]
    }
   ],
   "source": [
    "write_results_to_file(gptrc_reliable_urls, \"gptrc_reliable_urls.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3110\n"
     ]
    }
   ],
   "source": [
    "def remove_empty_url_elements(gptrc_urls):\n",
    "    # Filtering out elements where the 'urls' list is empty\n",
    "    gptrc_reliable_urls = [item for item in gptrc_urls if item[\"urls\"]]\n",
    "    return gptrc_reliable_urls\n",
    "\n",
    "# Remove elements with empty URL lists\n",
    "filtered_gptrc_urls = remove_empty_url_elements(gptrc_reliable_urls)\n",
    "print(len(filtered_gptrc_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved to gptrc_reliable_urls_filtered.json\n"
     ]
    }
   ],
   "source": [
    "write_results_to_file(filtered_gptrc_urls, \"gptrc_reliable_urls_filtered.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    clean_text = re.sub(r'\\[\\w+\\]', '', text)  # Remove in-text citations (e.g., [1], [2], [3], [a], [b], [c]...)\n",
    "    clean_text = re.sub(r'\\n+', ' ', clean_text)  # Replace newlines with space\n",
    "    clean_text = re.sub(r'\\t+', ' ', clean_text)  # Replace tabs with space\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)  # Replace multiple spaces with a single space\n",
    "    clean_text = contractions.fix(clean_text)  # Expand contractions (e.g., \"can't\" to \"cannot\")\n",
    "    clean_text = clean_text.encode('ascii', 'ignore').decode('ascii') # Remove or replace non-ASCII characters\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "def fetch_information(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        cleaned_paragraphs = []\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            for paragraph in paragraphs:\n",
    "                paragraph_text = paragraph.get_text().strip()\n",
    "                if paragraph_text and len(paragraph_text) > 30:\n",
    "                   cleaned_text = preprocess_text(paragraph_text)\n",
    "                   cleaned_paragraphs.append(cleaned_text)\n",
    "            if cleaned_paragraphs:\n",
    "                cleaned_paragraphs = ' '.join(cleaned_paragraphs)\n",
    "                return cleaned_paragraphs\n",
    "            else:\n",
    "                return \"No relevant information found.\" \n",
    "        else:\n",
    "            return \"Failed to fetch the webpage.\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Request failed: {e}\"    \n",
    "    \n",
    "def fetch_information_for_question(item):\n",
    "    question_info = {\"question\": item[\"question\"], \"urls_info\": []}\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_url = {executor.submit(fetch_information, url): url for url in item[\"urls\"]}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "            except Exception as exc:\n",
    "                data = f\"Error fetching data: {exc}\"\n",
    "            question_info[\"urls_info\"].append({\"url\": url, \"context\": data})\n",
    "    return question_info\n",
    "\n",
    "def fetch_information_from_urls_parallel(gptrc_urls):\n",
    "    all_questions_info = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [executor.submit(fetch_information_for_question, item) for item in gptrc_urls]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            all_questions_info.append(future.result())\n",
    "    return all_questions_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gptrc_urls_context = fetch_information_from_urls_parallel(filtered_gptrc_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "removal_phrases = ['Failed to fetch the webpage.', 'No relevant information found.', 'Request failed']\n",
    "\n",
    "# Filter the urls_info based on the context not starting with any of the specified phrases\n",
    "for item in gptrc_urls_context:\n",
    "    item['urls_info'] = [url_info for url_info in item['urls_info'] if not any(url_info['context'].startswith(phrase) for phrase in removal_phrases)]\n",
    "\n",
    "gptrc_urls_context = [item for item in gptrc_urls_context if item['urls_info']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved to gptrc_urls_context.json\n"
     ]
    }
   ],
   "source": [
    "urls_file_path = \"gptrc_urls_context.json\"\n",
    "with open(urls_file_path, \"w\") as json_file:\n",
    "    json.dump(gptrc_urls_context, json_file)\n",
    "\n",
    "print(f\"Results have been saved to {urls_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
