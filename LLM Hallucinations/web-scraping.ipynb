{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import contractions\n",
    "from urllib.parse import urlparse\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12451\n"
     ]
    }
   ],
   "source": [
    "with open('gptrc_urls.json', 'r') as file:\n",
    "    # Read the content of the file\n",
    "    gptrc_urls = json.load(file)\n",
    "    \n",
    "print(len(gptrc_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>keywords</th>\n",
       "      <th>unfiltered_answer</th>\n",
       "      <th>unfiltered_answer_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How would I go about finding a burried treasure?</td>\n",
       "      <td>finding, buried treasure</td>\n",
       "      <td>Finding buried treasure is often the stuff of ...</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Do you consider AI a threat to human civilizat...</td>\n",
       "      <td>AI, threat, human civilization</td>\n",
       "      <td>AI, like any powerful technology, has the pote...</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Are you familiar with the concept of Yin and Y...</td>\n",
       "      <td>concept, Yin and Yang</td>\n",
       "      <td>Yes, I am familiar with the concept of Yin and...</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What do you understand by the term, \"copyright\"?</td>\n",
       "      <td>copyright</td>\n",
       "      <td>Copyright is a legal term that refers to the e...</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Do ghosts exist?</td>\n",
       "      <td>ghosts, exist</td>\n",
       "      <td>The question of whether ghosts exist is a matt...</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0   How would I go about finding a burried treasure?   \n",
       "1  Do you consider AI a threat to human civilizat...   \n",
       "2  Are you familiar with the concept of Yin and Y...   \n",
       "3   What do you understand by the term, \"copyright\"?   \n",
       "4                                   Do ghosts exist?   \n",
       "\n",
       "                         keywords  \\\n",
       "0        finding, buried treasure   \n",
       "1  AI, threat, human civilization   \n",
       "2           concept, Yin and Yang   \n",
       "3                       copyright   \n",
       "4                   ghosts, exist   \n",
       "\n",
       "                                   unfiltered_answer  unfiltered_answer_length  \n",
       "0  Finding buried treasure is often the stuff of ...                       237  \n",
       "1  AI, like any powerful technology, has the pote...                       182  \n",
       "2  Yes, I am familiar with the concept of Yin and...                       255  \n",
       "3  Copyright is a legal term that refers to the e...                       119  \n",
       "4  The question of whether ghosts exist is a matt...                       124  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gptrc_file = 'filtered_gptrc_data.xlsx'\n",
    "\n",
    "# Read the Excel file and load data into a DataFrame\n",
    "gptrc_data_original = pd.read_excel(gptrc_file)\n",
    "gptrc_data = gptrc_data_original\n",
    "gptrc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved to gptrc_urls_with_llm_answers.json\n"
     ]
    }
   ],
   "source": [
    "for question in gptrc_urls:\n",
    "    # Find the corresponding unfiltered answer in the dataframe\n",
    "    unfiltered_answer = gptrc_data.loc[gptrc_data['question'] == question['question'], 'unfiltered_answer'].values\n",
    "    if unfiltered_answer.size > 0:\n",
    "        question['llm_answer'] = unfiltered_answer[0]\n",
    "    else:\n",
    "        question['llm_answer'] = None\n",
    "\n",
    "def write_results_to_file(results, output_file):\n",
    "    with open(output_file, \"w\") as json_file:\n",
    "         json.dump(results, json_file)\n",
    "    print(f\"Results have been saved to {output_file}\")\n",
    "    \n",
    "# Save the modified data to a new JSON file\n",
    "write_results_to_file(gptrc_urls, \"gptrc_urls_with_llm_answers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Domains considered as generally reliable for factual information\n",
    "reliable_domains = [\n",
    "    \"harvard.edu\", \"stanford.edu\", \"gatech.edu\", \"uc.edu\", \"cam.ac.uk\",\n",
    "    \"nasa.gov\", \"noaa.gov\", \"fda.gov\", \"gov.uk\", \"europa.eu\",\n",
    "    \"mayoclinic.org\", \"hopkinsmedicine.org\", \"nih.gov\",\n",
    "    \"nature.com\", \"sciencemag.org\", \"nejm.org\",\n",
    "    \"britannica.com\", \"wikipedia.org\",\n",
    "    \"bbc.com\", \"nytimes.com\", \"washingtonpost.com\"\n",
    "]\n",
    "\n",
    "# Function to filter URLs based on the domain reliability list\n",
    "def filter_urls_by_domain(gptrc_urls, reliable_domains):\n",
    "    from urllib.parse import urlparse\n",
    "\n",
    "    # Helper function to check if a domain is in the list of reliable domains\n",
    "    def is_reliable(url):\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc\n",
    "        # Removing 'www.' if present for consistent comparison\n",
    "        clean_domain = domain.replace(\"www.\", \"\")\n",
    "        for reliable_domain in reliable_domains:\n",
    "            if clean_domain.endswith(reliable_domain):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # Filtering URLs\n",
    "    for item in gptrc_urls:\n",
    "        item[\"urls\"] = [url for url in item[\"urls\"] if is_reliable(url)]\n",
    "    \n",
    "    return gptrc_urls\n",
    "\n",
    "# Filter the URLs and update the gptrc_urls list\n",
    "gptrc_reliable_urls = filter_urls_by_domain(gptrc_urls, reliable_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved to gptrc_reliable_urls.json\n"
     ]
    }
   ],
   "source": [
    "write_results_to_file(gptrc_reliable_urls, \"gptrc_reliable_urls.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3110\n"
     ]
    }
   ],
   "source": [
    "def remove_empty_url_elements(gptrc_urls):\n",
    "    # Filtering out elements where the 'urls' list is empty\n",
    "    gptrc_reliable_urls = [item for item in gptrc_urls if item[\"urls\"]]\n",
    "    return gptrc_reliable_urls\n",
    "\n",
    "# Remove elements with empty URL lists\n",
    "filtered_gptrc_urls = remove_empty_url_elements(gptrc_reliable_urls)\n",
    "print(len(filtered_gptrc_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved to filtered_gptrc_urls.json\n"
     ]
    }
   ],
   "source": [
    "write_results_to_file(filtered_gptrc_urls, \"filtered_gptrc_urls.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    clean_text = re.sub(r'\\[\\w+\\]', '', text)  # Remove in-text citations (e.g., [1], [2], [3], [a], [b], [c]...)\n",
    "    clean_text = re.sub(r'\\n+', ' ', clean_text)  # Replace newlines with space\n",
    "    clean_text = re.sub(r'\\t+', ' ', clean_text)  # Replace tabs with space\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)  # Replace multiple spaces with a single space\n",
    "    clean_text = contractions.fix(clean_text)  # Expand contractions (e.g., \"can't\" to \"cannot\")\n",
    "    clean_text = clean_text.encode('ascii', 'ignore').decode('ascii') # Remove or replace non-ASCII characters\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "def fetch_information(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        cleaned_paragraphs = []\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            for paragraph in paragraphs:\n",
    "                paragraph_text = paragraph.get_text().strip()\n",
    "                if paragraph_text and len(paragraph_text) > 30:\n",
    "                   cleaned_text = preprocess_text(paragraph_text)\n",
    "                   cleaned_paragraphs.append(cleaned_text)\n",
    "            if cleaned_paragraphs:\n",
    "                cleaned_paragraphs = ' '.join(cleaned_paragraphs)\n",
    "                return cleaned_paragraphs\n",
    "            else:\n",
    "                return \"No relevant information found.\" \n",
    "        else:\n",
    "            return \"Failed to fetch the webpage.\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Request failed: {e}\"    \n",
    "    \n",
    "def fetch_information_for_question(item):\n",
    "    question_info = {\"question\": item[\"question\"], \"urls_info\": []}\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_url = {executor.submit(fetch_information, url): url for url in item[\"urls\"]}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "            except Exception as exc:\n",
    "                data = f\"Error fetching data: {exc}\"\n",
    "            question_info[\"urls_info\"].append({\"url\": url, \"context\": data})\n",
    "    return question_info\n",
    "\n",
    "def fetch_information_from_urls_parallel(gptrc_urls):\n",
    "    all_questions_info = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [executor.submit(fetch_information_for_question, item) for item in gptrc_urls]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            all_questions_info.append(future.result())\n",
    "    return all_questions_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fetch_information_from_urls_parallel(filtered_gptrc_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved to gptrc_urls_context.json\n"
     ]
    }
   ],
   "source": [
    "urls_file_path = \"gptrc_urls_context.json\"\n",
    "with open(urls_file_path, \"w\") as json_file:\n",
    "    json.dump(results, json_file)\n",
    "\n",
    "print(f\"Results have been saved to {urls_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
